import feedparser
import requests
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore, storage
from datetime import datetime
import hashlib
import os
import json

print("ğŸ“¦ ì‹œì‘: Firebase ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •")

# Firebase ì´ˆê¸°í™”
with open("serviceAccountKey.json") as f:
    cred_dict = json.load(f)

cred = credentials.Certificate(cred_dict)
firebase_admin.initialize_app(cred, {
    'storageBucket': os.environ.get('FIREBASE_STORAGE_BUCKET')
})

db = firestore.client()
bucket = storage.bucket()

headers = {
    'User-Agent': 'Mozilla/5.0 (compatible; MyNewsBot/1.0; +http://example.com/bot)'
}

def parse_feed(url):
    print(f"ğŸŒ RSS ìš”ì²­ ì¤‘: {url}")
    res = requests.get(url, headers=headers, timeout=5)
    feed = feedparser.parse(res.content)
    return feed.entries

def extract_article_data(url):
    print(f"ğŸ“° ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§: {url}")
    res = requests.get(url, headers=headers, timeout=5)
    soup = BeautifulSoup(res.content, 'html.parser')
    content_el = soup.select_one('.art_body')
    img_el = soup.select_one('.art_photo img')
    content = content_el.get_text(strip=True) if content_el else ''
    img_url = img_el['src'] if img_el else None
    return content, img_url

def upload_image(img_url):
    print(f"ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ: {img_url}")
    res = requests.get(img_url)
    filename = hashlib.md5(img_url.encode()).hexdigest() + '.jpg'
    blob = bucket.blob(f'news_thumbnails/{filename}')
    blob.upload_from_string(res.content, content_type='image/jpeg')
    blob.make_public()
    return blob.public_url

def upload_to_firestore(title, link, content, image_url, published):
    print(f"âœ… Firestore ì €ì¥: {title}")
    doc_ref = db.collection('news').document()
    doc_ref.set({
        'title': title,
        'url': link,
        'content': content,
        'thumbnail': image_url,
        'published': published,
        'createdAt': datetime.utcnow()
    })

def main():
    rss_urls = [
        "https://sports.khan.co.kr/rss/soccer_korea-soccer",
        "https://sports.khan.co.kr/rss/soccer_world-soccer"
    ]

    all_entries = []
    for url in rss_urls:
        entries = parse_feed(url)
        print(f"ğŸ“Œ {url} ì—ì„œ {len(entries)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")
        all_entries.extend(entries)

    print(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {len(all_entries)}")

    for entry in all_entries[:10]:  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œ ì œí•œ
        try:
            print(f"â–¶ï¸ ê¸°ì‚¬ ì²˜ë¦¬: {entry.title}")
            content, img_url = extract_article_data(entry.link)
            image_url = upload_image(img_url) if img_url else ""

            published = getattr(entry, 'published', '')  # â† í•µì‹¬ ìˆ˜ì • í¬ì¸íŠ¸

            upload_to_firestore(
                entry.title,
                entry.link,
                content,
                image_url,
                published
            )
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {entry.title}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {entry.link} â†’ {e}")

# ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    print("ğŸ”¥ rss_crawler.py ì‹¤í–‰ ì‹œì‘")
    main()
