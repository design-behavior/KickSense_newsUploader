import feedparser
import requests
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore, storage
from datetime import datetime
import hashlib
import os
import json

print("ğŸ“¦ Firebase ì´ˆê¸°í™” ì‹œì‘")

# âœ… ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ë¡œë“œ
with open("serviceAccountKey.json") as f:
    cred_dict = json.load(f)

cred = credentials.Certificate(cred_dict)
firebase_admin.initialize_app(cred, {
    'storageBucket': os.environ.get('FIREBASE_STORAGE_BUCKET')
})

db = firestore.client()
bucket = storage.bucket()

headers = {
    'User-Agent': 'Mozilla/5.0 (compatible; MyNewsBot/1.0; +http://example.com/bot)'
}

def parse_feed(url):
    print(f"ğŸŒ RSS ìš”ì²­ ì¤‘: {url}")
    res = requests.get(url, headers=headers, timeout=5)
    feed = feedparser.parse(res.content)
    return feed.entries

def extract_article_data(url):
    print(f"ğŸ“° ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§: {url}")
    res = requests.get(url, headers=headers, timeout=5)
    soup = BeautifulSoup(res.content, 'html.parser')
    content_el = soup.select_one('.art_body')
    img_el = soup.select_one('.art_photo img')
    content = content_el.get_text(strip=True) if content_el else ''
    img_url = img_el['src'] if img_el else None
    return content, img_url

def upload_image(img_url):
    print(f"ğŸ–¼ ì´ë¯¸ì§€ ì—…ë¡œë“œ: {img_url}")
    res = requests.get(img_url)
    filename = hashlib.md5(img_url.encode()).hexdigest() + '.jpg'
    blob = bucket.blob(f'news_thumbnails/{filename}')
    blob.upload_from_string(res.content, content_type='image/jpeg')
    blob.make_public()
    return blob.public_url

def upload_to_firestore(title, link, content, image_url, published, category):
    print(f"âœ… Firestore ì €ì¥ ì‹œë„: {title} [{category}]")
    # ì¤‘ë³µ í™•ì¸
    existing = db.collection('news').where('url', '==', link).get()
    if existing:
        print(f"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: {link}")
        return  # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€
        
    doc_ref = db.collection('news').document()
    doc_ref.set({
        'title': title,
        'url': link,
        'content': content,
        'thumbnail': image_url,
        'published': published,
        'category': category,         # âœ… í•„ìˆ˜: êµ­ë‚´/í•´ì™¸ êµ¬ë¶„ í•„ë“œ!
        'createdAt': datetime.utcnow()
    })

def main():
    rss_sources = [
        ("https://sports.khan.co.kr/rss/soccer_korea-soccer", "domestic"),
        ("https://sports.khan.co.kr/rss/soccer_world-soccer", "international")
    ]

    for rss_url, category in rss_sources:
        entries = parse_feed(rss_url)
        print(f"ğŸ“Œ {rss_url} ì—ì„œ {len(entries)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨ [{category}]")

        for entry in entries[:18]:  # í•„ìš”ì‹œ ì „ì²´ ì²˜ë¦¬
            try:
                print(f"â–¶ï¸ ê¸°ì‚¬ ì²˜ë¦¬: {entry.title}")
                content, img_url = extract_article_data(entry.link)
                image_url = upload_image(img_url) if img_url else ""
                published = getattr(entry, 'published', '')

                upload_to_firestore(
                    entry.title,
                    entry.link,
                    content,
                    image_url,
                    published,
                    category    # âœ… ì¶”ê°€
                )

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {entry.link} â†’ {e}")

if __name__ == "__main__":
    print("ğŸ”¥ rss_crawler.py ì‹¤í–‰ ì‹œì‘")
    main()
